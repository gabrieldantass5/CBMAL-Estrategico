# üì∫ Transcri√ß√£o: Meu Novo Workflow de Programa√ß√£o com IA (2026)
**Fonte**: [https://www.youtube.com/watch?v=zgxorh9LhiE](https://www.youtube.com/watch?v=zgxorh9LhiE)
**Data da Transcri√ß√£o**: 28/01/2026 16:35:00

---

## ü§ñ AUTO-RESUMO (PONTOS CHAVE)

1. **Stack de Ferramentas 2026**:
   - **Cursor**: Usado principalmente para *Autocomplete* (Tab).
   - **Claude Code**: Agente principal para codifica√ß√£o (modelo Opus 4.5).
   - **Gemini 3 Pro**: Melhor modelo para UI/UX e design visual.
   - **GPT 5.2 Codex**: Especialista em refatora√ß√µes longas e complexas que exigem muito contexto.

2. **Evolu√ß√£o do Fluxo**:
   - Abandono do "One-Shot" (tentar fazer tudo de uma vez).
   - Foco em **"Plan First"**: Planejar detalhadamente antes de executar.
   - **Parallel Agents**: Uso de *Git Worktrees* para rodar m√∫ltiplas tarefas em paralelo sem conflitos.

3. **Contexto √© Tudo**:
   - Uso de arquivos `.md` espec√≠ficos por feature para manter a "Fonte da Verdade" para a IA.
   - Skill de automa√ß√£o para atualizar esses documentos de contexto a cada PR.

---

## üí° IDEIAS PARA O ECOSSISTEMA CBMAL

1. **Implementar Feature-Based Docs**: Criar uma pasta `03_Gestao_Processos/Contextos_IA` e gerar um arquivo `.md` para cada grande projeto (ex: `revisao_taxas.md`, `dashboard_comando.md`). Isso evita "alucina√ß√µes" e economiza tokens ao focar no contexto certo.
2. **Adotar o Mantra "Plan First" no /conductor**: Refinar nossa skill `conductor` para que ela sempre exija um OK do usu√°rio no plano de arquitetura antes de tocar em qualquer arquivo.
3. **Screenshot-to-Dashboard**: Utilizar o Gemini 3 Pro (via nossa skill de dashboard) para analisar prints de pain√©is de business intelligence modernos e adaptar o visual para os padr√µes do CBMAL.
4. **Git Worktrees para o Antigravity**: Avaliar a integra√ß√£o de Branchlet ou fluxos de Worktree para permitir que voc√™ trabalhe em dois Of√≠cios ou dois Indicadores simultaneamente sem que um interfira no arquivo do outro.

---

## üìù CONTE√öDO √çNTEGRO
In this video, I'm going to break down my new AI coding workflow for the year of 20 26. I made this video in the past on my channel before, but the state of AI and how quickly things change. I'm always testing out a bunch of different tools and honestly just testing out the different skill sets and features that all the new AI coding agents are coming out with. And that's why I wanted to create this brand new video to talk about what I'm doing these days in January of 2026 compared to 6 months ago when I last made this type of video. Now just to provide a little bit of context and background of myself, I'm a coding YouTuber as you can see here. So I'm always keeping up with the latest tooling and I'm always testing tools out for my channel so I can give my opinions on the new tooling that are out there. But outside of YouTube, I also code a lot every single day because I'm also building my own startup called Yorby. Yorby is a social media marketing tool geared towards businesses to help them more easily find content inspiration on how to make high performing marketing content as well as creating create that high performing marketing social media content themselves. That is my primary focus. I spend probably 80 to 90 percent of my week strictly building that. And even before that, I spent the past 5 years building over 14 different apps. So I code a lot every single day for the past 5 years. And on top of all that, I also worked a day job for the past 5 years in big tech. So I am coding a lot every single day and I haven't stopped for many, many years. Now I'm not trying to preach that my coding workflow is the best way or the right way to do things. I think a lot of you might be surprised with how relatively simple and not complex my workflow is. I'm just a random dude on internet trying to share my learnings and what I do on a day to day basis and what has worked for me. Now let's get into it. We're going to splitting up this video into 2 primary sections. Number one is a deep dive into the actual tech stack and toolings that I use. And then afterwards we're going to go over some high level workflow of how I use the AI coding and toolings and just general strategies as well. Alright, so first, let's go over the tech stack that I use. So I'm still using cursor as my primary AI IDE, but I primarily use cursor for the tab to complete, you know, like when you're typing around and you just get these like tab to complete stuff, I still find that to be the best tab to complete that I've tested throughout various other apps and I like it the most. So that is what I primarily use cursor for and I know that they've introduced a lot of other really fancy bells and whistles like for example I know that cursor now has the ability for you to open up your app within a web browser directly within cursor and then you can point at things with the cursor mouse and then you can modify everything through there. I personally haven't played around with that all too much I still just stick with using Google Chrome Maybe I'm old school that way. I'm sure it's a great feature. I just haven't quite adopted that feature yet because I just quite haven't found the need to do so quite yet. And I know they've also introduced a lot of other things like debug mode, agent review, bug bot, all that stuff. But once again, I keep things relatively simple with my cursor usage, largely relying on the tap to complete. And the reason for that is because I do use Claude code as my primary AI coding agent, and we'll come back to that soon. But roughly 6 months ago, I was really, really almost exclusively using Claude code, but as of late, I actually have found myself using cursor a little bit more and using Claude code less exclusively. Don't get me wrong. I still do the vast majority of my AI agent to coding with Claude code, but I have found some actual genuine use cases for using cursors, AI agent to coding as well. And I've primarily been using cursor for the ability to use different models, primarily GPT, 5.2 Gemini 3 pro. And also they're new or not that new at the time of recording this, but their own in-house composer one model, because each of these models and my opinion have a different use case for. when I use them. So once again, like I said, Claude code primarily Claude Opus 4.5. That is my primary go to coding agent for agent to coding stuff. But every now and then when Claude code is not able to solve a certain problem, I just want to get different opinions from different models. So I've been testing out this GPT 5.2 codecs and I've been seeing pretty good performance. Now, at least from my experience and just from what I read on the internet, GPT5 0.2 codecs, it is slow. It can be a very, very slow model, but that's why I don't use it for my day to day coding. And instead, when I have something like a really large refactor that I need to do, I like to delegate this task for GPT5 0.2 codecs because at least from what I've read and a little bit of personal experience, I've heard that GPT5 0.2 codecs is better for those really long running tasks that require a ton of context and can run for like many, many minutes, if not a few hours at a time. So I primarily use GPTE5 0.2 when I'm doing something that I know is very time intensive, requires a ton of code changes, but it's a little bit too slow for me to use in my day-to-day coding changes just because, you know, it kind of gets me out of my groove. Now for when I do want a bit of quicker and faster response time to keep myself in like the coding groove a little bit better, that's when I actually use Composer One's model. I believe Composer One is based off of the open weight GLM model. Don't quote me on that, but I believe that is the case. What I really like about Composer One is the fact that it is really fast and you still get pretty good performance. Now, obviously because it thinks and processes a lot more slowly compared to like the Opus 4.5, the 5.2 codexes, or even the Gemini 3 pros. I do think like the total raw intelligence is less, but at the trade off it comes with is the faster response time and the faster output tokens per second or time the first token. And I really like that as it helps me stay in the groove a lot better in my coding flow. So I have been mixing that in a little bit as well. And then last but not least, I do love Gemini. I'm a Gemini diehard. I've been strictly Gemini 3 pro. I think it is a step function above any other models out there in terms of UI and UX performance. Even if I give it really vague instructions, I'm just like, make this look better, make this prettier. Gemini 3 Pro's output, in my opinion for my taste, is substantially better than Claude or GPT or Composer. So that is what I primarily use Gemini 3 Pro for. So that's kinda how I use all of the different models on a day-to-day basis, specifically within Cursor. And pretty much every single time, I always go with plan first and then execute. I pretty rarely ever just go raw dog execute. I'm like, whoa, whoa, whoa, show me what you're going to do first. Let me develop some understanding about it and then let's start moving forward with it. I also think it's a little bit telling because I also remember reading a paper published by Anthropic a couple of months ago, maybe a year ago at this point, where they said that just by telling the model to wait, and think just for like an arbitrary amount of time. It significantly improved the output performance. Now I don't know how true that is. I'm just trusting Anthropic as an LLM expert on their opinion on that. So ever since I read that article, I almost always default to using any plan mode first just to make sure it has sufficient time to gather the context. Think I often find that it will implement a better quality solution. So that's kind of how I use Cursor. I know that there's probably a lot of alpha in terms of using all the various debug modes. I've heard really good things about it using the built-in browser within Cursor. I think that could be really compelling as well. I just haven't quite adopted it yet. I don't think they're bad features. I think they're good features. It's just, you know, I'm kind of a boomer at this point and it's a little bit harder to break out of my old existing workflow and my habits. But I do want to try out going all in on Cursor again and exclusively using Cursor and seeing how that goes. Now outside of cursor, I've already spoiled it a little bit earlier in this video, but I did talk about how cloud code today is still my primary go-to way of doing the vast majority of my agent and coding. I still go back and forth a little bit between whether I like doing it within like the terminal view down here, but I've also been using a lot of the VS code extension. I actually like the, you know, the UI is pretty good. I flip-flop back and forth depending how I'm feeling these days. Right now, at the time of filming this video, I am back to the VS Code extension rather than the raw dog terminal. I do think the readability of the plan is easier. I don't really think there's a difference in performance per se, but I just think strictly from a UI UX perspective, I still do use Cloud Code. Now, in terms of how I use Claude code though, obviously there are some crazy people out there using Claude code to like the absolute bleeding edge. I wouldn't say I'm quite there yet. So for example, I know Claude has a lot of skills or agents and stuff that you can build in and add into your Claude code. Personally, I don't do a lot of that. I often feel like it's kind of like that Midwit meme. Like I'm talking about this meme right here where on the left on like the bumbling idiot and then the awakened Jedi over here is like, just tell AI to write code for you. And then the middle is like orchestrator, agent skills, parallel agents, all coding and checking out at the same time. I feel like, I don't know, maybe I'll be wrong about this and I'll like bite my words for this, but I don't want to be that guy stuck in the middle over complicating my AI workflow. I try to keep things really simple with my cloud code usage and I just tell it to build. So in general with cloud code, I always use Opus four point five and I am paying for the two hundred dollars a month, 20 X max plan, which is very expensive, but that gets me the absolute most usage. So I never have to worry about running out of usage and getting rate limited. And once again, I almost always default to using plan mode before implementing. Now within Cloud Code, I do have one skill in particular that I do use a lot. Obviously you can see I have like a ton of these different like skills. I don't remember adding a lot of these. I think a lot of them are kind of just like built in already. But the one that I do use a lot is this one, the PR Creator with Docs. So essentially this is a skill that I also had AI help me write that helps me keep the context up to date throughout my entire application. And the way that I do that, essentially what the skill does is whenever I tell cloud code to make a PR, it'll execute this skill and the skill will then analyze all of the file changes that I made in the active branch. And then from there, it'll update. any necessary clod.md files to keep the context up to date. So as you can see, I have a ton of different clod.md files and essentially I create clod.md file for every major feature that the app has. So that clod.md file becomes the major source of truth for that particular feature. So then with that one skill that I have, the PR creator with docs, I think that's what it was called. It'll look for any clod.md files for the specific features that I modified and keep those up to date over and over again. This has been a really big to unlock for me, because I, like you all know, for maximum and best LLM usage and output, context is key, and I hate having to tag a bunch of files every single time I'm working on a repeat feature, and just creating these Claude.md files and making sure they get updated with every single code change that I ever do, at least for me, in my personal experience, has significantly decreased the amount of manual files that I have to tag within Claude, and it has increased that almost magical experience that Claude code gives me in terms of making the changes and knowing where all the changes live. It's a very small skill, nothing much, but it has made a huge difference in my opinion. And then last but not least, we'll talk about the terminal that I use. Once again, I do the vast majority of my work within cursor, but I do use warp as my go-to terminal of choice. I've been using them since like, 20, 21. This video is not sponsored by them. I'm just sharing the tools that I'm using. But in the recent years, they really dive really deep into a lot more like AI agents coding stuff and they have a pretty good agent to coding experience built into it. So with warp, if I ever need to do some very complex terminal command, I'll often just tell it in natural language and then warp will then turn that natural language into actual terminal commands. And they also have some pretty good, lightweight, agented coding stuff that you can do directly within the terminal that I do every now and then if I know the change is really small and I don't really want to open up cursor, for example. So that's kind of a deep dive into the actual tech stack that I used the last 6 months that I made the video to this video talking about my AI coding workflow. The actual tools that I'm using honestly haven't changed that much, but the way that I actually use the tools on a day-to-day basis have changed, as I explained throughout the video. So now let's kind of transition over into like a more higher level overview about how exactly I use AI and how I use AI has changed. So the first big change that I want to talk about is parallel agents. So right now it's really in vogue to run a lot of parallel agents at once, especially with models like the GPT5 0.2 codexes, Gemini 3 pros and Opus 4 point fives that are pretty slow and you get really good performance, but oftentimes you can be stuck there waiting a couple of minutes for a change to complete. So rather than twiddling my thumbs and doing brain rot doom scrolling on my phone, I decided, hey, let's do parallel agents and do parallel work. And this is what a lot of other people have been doing as well. So here is exactly how I structure my parallel agent workflow. Now, specifically, I use this tool. Once again, I'm not sponsored by them. It's called the branchlet. It is a open source like DLI Git work tree manager. So essentially what you can do is over here, you can see I have my Yorby schema one and Yorby schema one and dot work tree. So within Yorby schema one, I will go do branchlet. And for those of you that don't know, like GitHub work trees is a way to work in completely separate branches that have their own unique file set of changes and tracks their own set of changes so that you don't have to worry about any merge conflicts if you're working on 2 or 3 different tasks at once all in one. branch, all the changes are isolated into their own file directory. And this has become really useful in my day-to-day work, especially when using parallel AI agent work, working across various different changes. So within branchlet, you can list all changes, you can create new work trees directly from the parent directory that you're in. Really lightweight tool and it has really improved my day-to-day coding usage. So for example, when you run a branchlet and you create a new branch, for example, within Yorby schema one, it then creates this Yorby schema one dot work tree directory. And that's where all the sub work trees of the parent Yorby schema one are defined in there. And then I just open up a separate cursor instance and just start going to town, making changes there. As you can see, I have this other directory called Yorby Schema 2. Now this is particularly like very specific towards working on a super base application, which is my app is built on. I use Next.js with super base running my background. I primarily build Next.js with super base running the entire backend essentially. Now with SuperBase, the way that you run your local SuperBase instance is you essentially create your own local SuperBase Docker image and you run that Docker container and then your local app connects to that Docker container. The reason why I had this Yorby schema 2 directory is because sometimes I want to work on 2 separate tasks that require their own database changes. And I just wanna have these database changes completely isolated from one another and I don't want them to like overwrite or conflict with each other. That's why I created this Yorby schema 2 directory strictly designed for all work. own database changes outside of the URB Schema One. And then I modify my local SuperBase instance for URB Schema 2 to have different ports assigned to it compared to URB Schema One so that I can have different apps connecting to their own individual instances of my local SuperBase instance so I can be free to make any database changes without having to worry about conflicts arising. So that's how I've been tackling parallel agents and it has been phenomenal. It has been so much more productive being able to work on multiple things at once, ten out of 10. A little bit of a side note, but I have been testing out anti-gravity and I think anti-gravity has so much potential because I love the UI that it has for like that agent inbox thing that you see right here. Like this UI is great, but the big downside right now is the fact that they don't support GitHub work trees and then you can't really work on changes isolated in their own space. All the changes are gonna be merged and conflicting with each other all in the same branch. This makes it not as efficient and not as great So I do think once anti-gravity kind of gets that set up, I think it'll unlock a lot of stuff, a lot of potential. So I am looking out for that. I also do know Cursor does have their agent mode as well. Haven't quite used it all that much, but I think it could be useful. I need to play around with that a little bit more. Next up, I really try my best not to one-shot anything anymore. Like don't get me wrong, I think that the models are capable of one-shotting features to a certain extent, but I find myself like with when you're trying to one-shot a particular feature being built, It's like I tell the model, hey, build me a car. And previously in the old school way of coding, you'd be like, okay, you want to build a car. Let's first create the engine. Let's create the tires. Let's attach the tires to the engine. Let's create the frame piece by piece, building up to the final product of the car. But right now with one shotting, I feel like I tell it to do something and I say, hey, build me a car. And then it'll build me a flying car with machine guns attached to it with 50 wheels and 50 different sound systems going on at once. I have to peel back every single feature to get it as bare bones and as simple as possible. wrong. It works, but oftentimes I do find the code sometimes not be implemented exactly how I wanted. And I know some people in the comments are going to be like, bro, this is a skill issue. You just go to write better specs, spectrum and development, bro. Look, I've tried it all. Once again, it might actually be a skill issue. I could be just like a trash developer. Don't get me wrong. I'm not disagreeing with you there, but I also do think like this wasn't a problem for me when my code base was small. But as my code base has gotten larger and larger and I build more features and it's just gotten more complex, I found it much harder for the models to handle one-shotting anything these days. So I really, really try my best not to one-shot anything. Obviously I still find myself being a little crazy when I'm like, hey, build out this entire feature one shot, I still find myself doing that. Mixed results, whether it really works or not, I really try my best not to just try to one shot anything and I try to just do it, building everything like one step at a time, piece by piece from the ground up, really just trying to delegate the actual code writing to the LLMs while I still maintain and handle a lot of the architectural decisions myself. I feel like one shotting and then peeling back all the features and trying to fix the one shot results makes me waste more time than if I were to just try to build it up piece by piece correctly on the first go. All right, now let's talk about how I create better UI within my apps. Once again, I'm not a UI expert. My apps aren't actually gorgeous. Like they're honestly like they're okay UI wise. This is really just talking about how to get better UI output from your models. And the way that I have primarily been doing this is I find the various apps and websites that I do like in terms of their design language and I just create a bunch of screenshots of them, the landing page, the actual app itself. And then like I mentioned earlier, I do find Gemini 3 to be a, the best UI model. I then dump all of these models into Gemini 3 pro and say, Hey, take all these images and create like a universal design language for myself that I can use within my app. And then from that design language and design system output, I then import it into a separate Claude dot MD file within my app. And then whenever I need to make any UI changes, I just make sure to reference that Claude dot MD file. for all the UI work that gets done. Once again, not pro level design stuff, but it is powerful to let someone like me, a traditionally trained engineer, to have like a six out of ten design output, whereas previously before all this, I was getting like 2 out of 10, 3 out of ten design output. It's kind of similar to how AI coding tools allows designers and literally anybody in the world build apps, not at like a ten out of ten developer level, but at like a five out of 10, 6 out of ten developer level. So, and I still find that to be one of the biggest drawbacks of the whole AI driven development, just like, trying to create good design. That is something that I've really been trying to focus on making better. And Gemini 3 was a good step in the right direction. And then creating this like design system documentation with a bunch of screenshots has been another step in the right direction. So getting better and better. And I'm still trying to crack that part of my workflow better. So we'll see how much more progress I can make there. That's been something that I've really been trying to focus on as of late. And another part of my AA workflow that I've been trying to work on is actually incorporating more node code slash load code tools in our tech stack in general. And there's honestly a lot of reasons for this. Number one is the fact that our team is expanding and we have some technical and non-technical people as well. And I think it's important to let non-technical people also contribute to the product development. And while it may not be with code, they definitely could help out with building no code automations. And also in general, I do think that using no code tools to power certain parts of your company or your app is actually faster than building everything out from scratch. And velocity is something that I'm really focusing a lot on this coming year. I used to think that velocity meant locking in, grinding for hours and hours and hours, building everything completely from scratch. But honestly, that's really not the case. And really what I found is that true velocity is just using whatever is the best and fastest solution out there and to not reinvent the wheel because that is just not necessary. And I've recently been using MIME Studio, who's also the sponsor of today's video, to start powering huge portions of my app. For example, within Yorby, we have our viral content database, which is a hand-picked, hand-curated set. of proven viral videos that other businesses have posted on social media to go viral and market their business and market their product. And this is actually powered by MindStudio. This entire workflow of pulling all the data, categorizing it, and saving the data into our actual SuperBase database is actually all powered by MindStudio. And don't get me wrong, we could build this out from scratch ourselves, but that requires a lot of overhead of creating a really complex workflow management process, but by using MindStudio, we were able to create the first version of this feature way faster than if we had tried to build it out ourselves, and we liked it so much that we continued to use it to power this entire flow in our app. It's powerful enough for me, a developer, to go in and do a lot of technical things with it, but it's also simple enough that non-technical people can jump in and create automations for themselves. If you want to try it out yourself, you can go visit MindStudio and they have a completely free plan that you can try yourself. And once again, thank you to MindStudio for sponsoring this portion of the video. But that is a quick overview about how I use everything and how I code with AI. But I'm also curious to hear how you code with AI. Leave me some comments down below. Leave any questions that you may have. But I also want to hear about what your coding workflow is. Is there a certain tool that I'm not trying out that you find really useful? Is the Ralph Wiggum's method really that good? Let me know in the comments. I want to hear some thoughts on that. But that's all I got for today's video. Thanks so much for watching and I'll see you in the next

---
*Gerado automaticamente pelo Cockpit Estrat√©gico CBMAL.*
